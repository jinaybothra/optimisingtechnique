{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "-Gvs5iM26Kzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "68lavpLK6t66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Data using sklearn"
      ],
      "metadata": {
        "id": "bhO0i6Wh7IiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic dataset with 2 features and 2 classes\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, n_clusters_per_class=1, n_redundant=0, random_state=42)\n",
        "n_features=2\n",
        "\n",
        "\n",
        "def normalise(X):\n",
        "    u = X.mean(axis=0)\n",
        "    std = X.std(axis=0)\n",
        "\n",
        "    return (X-u)/std\n",
        "X=normalise(X)\n",
        "\n",
        "def addExtraColumn(X):\n",
        "    if X.shape[1] == n_features:\n",
        "        ones = np.ones((X.shape[0],1))\n",
        "        X = np.hstack((ones,X))\n",
        "\n",
        "    return X\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train=addExtraColumn(X_train)\n",
        "X_test=addExtraColumn(X_test)"
      ],
      "metadata": {
        "id": "q-xgLFuY7H40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Logit Model"
      ],
      "metadata": {
        "id": "jj_xKEXh7l0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AnHCFBI5fXJ"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionCustom:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, regularization_param=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.regularization_param = regularization_param\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _compute_cost(self, X, y):\n",
        "        m = len(y)\n",
        "        h = self._sigmoid(np.dot(X, self.theta))\n",
        "        cost = (1/m) * np.sum((-y * np.log(h)) - ((1 - y) * np.log(1 - h)))\n",
        "        regularization_term = (self.regularization_param / (2 * m)) * np.sum(np.square(self.theta[1:]))\n",
        "        cost += regularization_term\n",
        "        return cost\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.theta = np.zeros(n)\n",
        "        for i in range(self.num_iterations):\n",
        "            z = np.dot(X, self.theta)\n",
        "            h = self._sigmoid(z)\n",
        "            gradient = (1/m) * np.dot(X.T, (h - y))\n",
        "            gradient[1:] += (self.regularization_param / m) * self.theta[1:]\n",
        "            self.theta -= self.learning_rate * gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.theta)\n",
        "        predictions=self._sigmoid(z)\n",
        "        return np.round(predictions)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        predictions = np.round(self.predict(X))\n",
        "        accuracy = np.mean(predictions == y)\n",
        "        return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objective Function\n"
      ],
      "metadata": {
        "id": "0P04-q7JPSBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_function(params, X_train, y_train, X_test, y_test):\n",
        "    learning_Rate = 10**params[0]\n",
        "    model = LogisticRegressionCustom(learning_rate=learning_Rate, num_iterations=1000, regularization_param=0.01)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return -accuracy  # Minimize negative accuracy"
      ],
      "metadata": {
        "id": "3qDlpbhzPU4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grey Wolf Optimization Algorithm"
      ],
      "metadata": {
        "id": "vbATc_OJ74hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grey Wolf Optimization Algorithm\n",
        "def grey_wolf_optimization(objective_function, dim=1, n_iter=100):\n",
        "    num_wolves = 5\n",
        "    lb, ub = -2, 2  # Lower and upper bounds for parameters (adjust according to your problem)\n",
        "    alpha, beta, delta = np.random.uniform(lb, ub, (num_wolves, dim)), np.random.uniform(lb, ub, (num_wolves, dim)), np.random.uniform(lb, ub, (num_wolves, dim))\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        a, b, d = 2 - 2 * _ / n_iter, 2 * _ / n_iter, 2 * _ / n_iter\n",
        "        for j in range(num_wolves):\n",
        "            X1, X2, X3 = alpha[j].copy(), beta[j].copy(), delta[j].copy()\n",
        "            for k in range(dim):\n",
        "                X1[k] = alpha[j][k] - a * np.abs(2 * np.random.rand() * alpha[j][k] - X1[k])\n",
        "                X2[k] = beta[j][k] - b * np.abs(2 * np.random.rand() * beta[j][k] - X2[k])\n",
        "                X3[k] = delta[j][k] - d * np.abs(2 * np.random.rand() * delta[j][k] - X3[k])\n",
        "            X_mean = (X1 + X2 + X3) / 3\n",
        "            if objective_function(X_mean, X_train, y_train, X_test, y_test) < objective_function(alpha[j], X_train, y_train, X_test, y_test):\n",
        "                alpha[j] = X_mean.copy()\n",
        "            elif objective_function(alpha[j], X_train, y_train, X_test, y_test) < objective_function(X_mean, X_train, y_train, X_test, y_test) < objective_function(beta[j], X_train, y_train, X_test, y_test):\n",
        "                beta[j] = X_mean.copy()\n",
        "            elif objective_function(beta[j], X_train, y_train, X_test, y_test) < objective_function(X_mean, X_train, y_train, X_test, y_test) < objective_function(delta[j], X_train, y_train, X_test, y_test):\n",
        "                delta[j] = X_mean.copy()\n",
        "\n",
        "    best_wolf = alpha[np.argmin([objective_function(alpha[i], X_train, y_train, X_test, y_test) for i in range(num_wolves)])]\n",
        "    return best_wolf\n",
        "\n"
      ],
      "metadata": {
        "id": "AqWQlCZR79mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Horse Herd Optimization Algorithm"
      ],
      "metadata": {
        "id": "NJo4Qd7A8Iro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Horse Herd Optimization Algorithm\n",
        "def horse_herd_optimization(objective_function, dim=1, n_iter=100):\n",
        "    num_horses = 5\n",
        "    lb, ub = -2, 2  # Lower and upper bounds for parameters (adjust according to your problem)\n",
        "    herd = np.random.uniform(lb, ub, (num_horses, dim))\n",
        "    for _ in range(n_iter):\n",
        "        for i in range(num_horses):\n",
        "            rand1, rand2 = np.random.rand(), np.random.rand()\n",
        "            A = 2 * rand1 - 1\n",
        "            r1, r2 = np.random.rand(), np.random.rand()\n",
        "            D = r1 * (herd[np.random.randint(num_horses)] - r2 * herd[np.random.randint(num_horses)])\n",
        "            herd[i] = np.clip(A * herd[i] + D, lb, ub)\n",
        "\n",
        "    best_horse = herd[np.argmin([objective_function(herd[i], X_train, y_train, X_test, y_test) for i in range(num_horses)])]\n",
        "    return best_horse"
      ],
      "metadata": {
        "id": "dJysfWwa8PHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating model"
      ],
      "metadata": {
        "id": "eyuCChgN8i6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Logistic Regression Model\n",
        "simple_lr =LogisticRegressionCustom(learning_rate=0.01, num_iterations=50, regularization_param=0.01)\n",
        "simple_lr.fit(X_train, y_train)\n",
        "simple_lr_accuracy = simple_lr.score(X_test, y_test)\n",
        "\n",
        "# Logistic Regression Model with Grey Wolf Optimization\n",
        "gwo_best_params = grey_wolf_optimization(objective_function, dim=1, n_iter=100)\n",
        "gwo_lr = LogisticRegressionCustom(learning_rate=10**gwo_best_params[0], num_iterations=50, regularization_param=0.01)\n",
        "gwo_lr.fit(X_train, y_train)\n",
        "gwo_accuracy = gwo_lr.score(X_test, y_test)\n",
        "\n",
        "# Logistic Regression Model with Horse Herd Optimization\n",
        "hho_best_params = horse_herd_optimization(objective_function, dim=1, n_iter=100)\n",
        "hho_lr = LogisticRegressionCustom(learning_rate=10**hho_best_params[0], num_iterations=100, regularization_param=0.01)\n",
        "hho_lr.fit(X_train, y_train)\n",
        "hho_accuracy = hho_lr.score(X_test, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "5SkpmffQ73Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Presenting Results"
      ],
      "metadata": {
        "id": "_vfHqQoe8o-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the comparison results\n",
        "print(\"Simple Logistic Regression Accuracy: {:.2f}%\".format(simple_lr_accuracy * 100))\n",
        "print(\"Logistic Regression with GWO Accuracy: {:.2f}%\".format(gwo_accuracy * 100))\n",
        "print(\"Logistic Regression with HHO Accuracy: {:.2f}%\".format(hho_accuracy * 100))"
      ],
      "metadata": {
        "id": "I-eFXo9n8sl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528fa194-78ab-4130-e185-7d2011b69643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Logistic Regression Accuracy: 89.00%\n",
            "Logistic Regression with GWO Accuracy: 89.50%\n",
            "Logistic Regression with HHO Accuracy: 90.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZGdOBJSQ6B5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}